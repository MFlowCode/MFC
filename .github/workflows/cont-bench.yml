name: Continuous Benchmarking

on:
  pull_request:
  workflow_dispatch:
  push:
    branches: [main, master]

permissions:
  contents: write
  deployments: write
  pages: write
  id-token: write

jobs:
  self:
    name: "Continuous Benchmarking"
    runs-on: ubuntu-latest
    steps:
      - name: Clone - PR
        uses: actions/checkout@v4
        with:
          path: pr

      - name: Setup
        run: | 
          sudo apt update -y
          sudo apt install -y cmake gcc g++ python3 python3-dev hdf5-tools \
                    libfftw3-dev libhdf5-dev openmpi-bin libopenmpi-dev
          export TOKEN=$(gh auth token)
          sudo wget -qO /usr/local/bin/yq https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64
          sudo chmod +x /usr/local/bin/yq
          yq --version

      - name: Run Fortran Benchmark
        run: |
          (cd pr     && ./mfc.sh bench -o bench.yaml)
          find pr/ -maxdepth 1 -name "*.yaml" -exec sh -c 'yq eval -o=json "$1" > "${1%.yaml}.json"' _ {} \;

      # - name: Store benchmark result
      #   uses: benchmark-action/github-action-benchmark@v1
      #   with:
      #     name: Fortran Benchmark
      #     tool: 'googlecpp'
      #     output-file-path: pr/bench*.json
      #     github-token: ${{ secrets.GITHUB_TOKEN }}
      #     auto-push: true
      #     alert-threshold: '200%'
      #     comment-on-alert: true
      #     fail-on-alert: true
      #     gh-pages-branch: 'gh-pages'
      #     benchmark-data-dir-path: benchmarks
      
      # - name: Create Benchmark Documentation
      #   run: |
      #     mkdir -p pr/docs/documentation
      #     cat > pr/docs/documentation/cont-bench.md << 'EOF'
      #     # Continuous Benchmarking

      #     This page provides an overview of MFC's continuous benchmarking system and results.

      #     ## Overview

      #     The continuous benchmarking system automatically runs performance tests on every approved pull request and main branch commit to track performance regressions and improvements over time.

      #     ## Benchmark Results

      #     ### Live Dashboard
          
      #     View the interactive benchmark dashboard with historical performance data:
          
      #     **[🔗 Live Benchmark Results](https://mflowcode.github.io/MFC/benchmarks/)**

      #     ### Key Metrics

      #     Our benchmarking system tracks the following performance metrics:

      #     - **Execution Time**: Overall runtime of benchmark cases
      #     - **Memory Usage**: Peak memory consumption during execution
      #     - **Computational Efficiency**: Performance per computational unit
      #     - **Scalability**: Performance across different problem sizes

      #     ### Benchmark Cases

      #     The benchmark suite includes:

      #     1. **Standard Test Cases**: Representative fluid dynamics problems
      #     2. **Scaling Tests**: Performance evaluation across different core counts
      #     3. **Memory Tests**: Memory efficiency and usage patterns
      #     4. **Accuracy Tests**: Verification of numerical accuracy

      #     ## Performance Trends

      #     ```mermaid
      #     graph LR
      #         A[PR Submitted] --> B[Benchmark Run]
      #         B --> C[Results Stored]
      #         C --> D[Performance Comparison]
      #         D --> E{Performance OK?}
      #         E -->|Yes| F[PR Approved]
      #         E -->|No| G[Alert Generated]
      #         G --> H[Developer Notified]
      #     ```

      #     ## Alert System

      #     The system automatically:

      #     - 🚨 **Generates alerts** when performance degrades by more than 200%
      #     - 📊 **Comments on PRs** with performance impact analysis
      #     - 📈 **Tracks trends** to identify gradual performance changes
      #     - 👥 **Notifies maintainers** of significant performance issues

      #     ## Configuration

      #     ### Benchmark Triggers

      #     Benchmarks are automatically triggered on:

      #     - ✅ Approved pull request reviews
      #     - 🔄 Pull requests from trusted contributors
      #     - 📦 Pushes to main/master branches
      #     - 🎯 Manual workflow dispatch

      #     ### Performance Thresholds

      #     - **Alert Threshold**: 200% performance degradation
      #     - **Fail Threshold**: Critical performance regressions
      #     - **Comparison Base**: Previous main branch performance

      #     ## Interpreting Results

      #     ### Performance Metrics

      #     | Metric | Description | Good Trend | Bad Trend |
      #     |--------|-------------|------------|-----------|
      #     | Runtime | Execution time | ⬇️ Decreasing | ⬆️ Increasing |
      #     | Memory | Peak memory usage | ⬇️ Decreasing | ⬆️ Increasing |
      #     | Efficiency | Ops per second | ⬆️ Increasing | ⬇️ Decreasing |

      #     ### Reading the Dashboard

      #     1. **Timeline View**: Shows performance evolution over time
      #     2. **Comparison View**: Compares current vs. baseline performance
      #     3. **Detailed Metrics**: Drill down into specific performance aspects
      #     4. **Regression Detection**: Automatically highlights performance issues

      #     ## Contributing to Benchmarks

      #     ### Adding New Benchmarks

      #     To add new benchmark cases:

      #     1. Add test case to `benchmarks/` directory
      #     2. Update benchmark configuration
      #     3. Ensure proper performance metrics collection
      #     4. Test locally before submitting PR

      #     ### Best Practices

      #     - 🎯 **Focus on representative cases** that reflect real-world usage
      #     - 📊 **Include scalability tests** for different problem sizes
      #     - 🔄 **Maintain benchmark stability** to ensure reliable comparisons
      #     - 📝 **Document benchmark purpose** and expected performance characteristics

      #     ## Troubleshooting

      #     ### Common Issues

      #     | Issue | Cause | Solution |
      #     |-------|-------|----------|
      #     | Benchmark timeout | Long-running test | Optimize test case or increase timeout |
      #     | Memory errors | Insufficient resources | Check memory requirements |
      #     | Inconsistent results | System variability | Multiple runs or statistical analysis |

      #     ### Getting Help

      #     - 📧 Contact: @sbryngelson for benchmark-related issues
      #     - 🐛 Issues: Report problems via GitHub issues
      #     - 📖 Documentation: Check MFC documentation for detailed guides

      #     ---

      #     *Last updated: $(date '+%Y-%m-%d %H:%M:%S UTC')*
      #     *Generated automatically by the continuous benchmarking workflow*
      #     EOF

      # - name: Commit Documentation
      #   if: github.event_name == 'push' && github.ref == 'refs/heads/main'
      #   run: |
      #     cd pr
      #     git config --local user.email "action@github.com"
      #     git config --local user.name "GitHub Action"
      #     git add docs/documentation/cont-bench.md
      #     if git diff --staged --quiet; then
      #       echo "No changes to commit"
      #     else
      #       git commit -m "Update continuous benchmarking documentation"
      #       git push
      #     fi
      
      # - name: Archive Results
      #   uses: actions/upload-artifact@v4
      #   if: always()
      #   with:
      #     name: benchmark-results
      #     path: |
      #       pr/bench-*
      #       pr/build/benchmarks/*
      #       pr/docs/documentation/cont-bench.md

  # deploy-pages:
  #   name: Deploy Benchmark Pages
  #   if: ${{ github.event_name == 'push' && github.ref == 'refs/heads/main' }}
  #   needs: self
  #   runs-on: ubuntu-latest
  #   environment:
  #     name: github-pages
  #     url: ${{ steps.deployment.outputs.page_url }}
  #   steps:
  #     - name: Checkout
  #       uses: actions/checkout@v4
  #       with:
  #         ref: gh-pages

  #     - name: Setup Pages
  #       uses: actions/configure-pages@v4
      
  #     - name: Upload artifact
  #       uses: actions/upload-pages-artifact@v2
  #       with:
  #         path: .

  #     - name: Deploy to GitHub Pages
  #       id: deployment
  #       uses: actions/deploy-pages@v3
