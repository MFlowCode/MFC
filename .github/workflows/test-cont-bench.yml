name: Continuous Benchmarking

on: [push, pull_request, workflow_dispatch]

permissions:
  contents: write
  deployments: write
  pages: write
  id-token: write

jobs:
  self:
    name: "Store benchmark result"
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Convert MFC to Google Benchmark Format
        run: |
          python3 << 'EOF'
          import json
          from datetime import datetime

          # Read the MFC benchmark data
          with open('bench.json', 'r') as f:
              mfc_data = json.load(f)

          # Convert to Google Benchmark format
          benchmarks = []

          for case_name, case_data in mfc_data['cases'].items():
              output_summary = case_data['output_summary']
              
              # Simulation execution time
              if 'simulation' in output_summary and 'exec' in output_summary['simulation']:
                  benchmarks.append({
                      "name": f"{case_name}/simulation_time",
                      "family_index": len(benchmarks),
                      "per_family_instance_index": 0,
                      "run_name": f"{case_name}/simulation_time",
                      "run_type": "iteration",
                      "repetitions": 1,
                      "repetition_index": 0,
                      "threads": 1,
                      "iterations": 1,
                      "real_time": output_summary['simulation']['exec'] * 1e9,
                      "cpu_time": output_summary['simulation']['exec'] * 1e9,
                      "time_unit": "ns"
                  })
              
              # Simulation grind time
              if 'simulation' in output_summary and 'grind' in output_summary['simulation']:
                  benchmarks.append({
                      "name": f"{case_name}/grind_time",
                      "family_index": len(benchmarks),
                      "per_family_instance_index": 0,
                      "run_name": f"{case_name}/grind_time",
                      "run_type": "iteration",
                      "repetitions": 1,
                      "repetition_index": 0,
                      "threads": 1,
                      "iterations": 1,
                      "real_time": output_summary['simulation']['grind'],
                      "cpu_time": output_summary['simulation']['grind'],
                      "time_unit": "ns"
                  })

          # Create Google Benchmark format
          google_benchmark_data = {
              "context": {
                  "date": datetime.now().isoformat(),
                  "host_name": "github-runner",
                  "executable": "mfc_benchmark",
                  "num_cpus": 2,
                  "mhz_per_cpu": 2000,
                  "cpu_scaling_enabled": False,
                  "caches": []
              },
              "benchmarks": benchmarks
          }

          # Write the converted data
          with open('bench-google.json', 'w') as f:
              json.dump(google_benchmark_data, f, indent=2)

          print(f"✓ Converted {len(benchmarks)} benchmark measurements")
          EOF
    
      - name: Create report generator script
        run: |
          cat > generate_report.py << 'SCRIPT'
          import json
          import os
          from datetime import datetime

          def generate_markdown_report():
              # Read benchmark data
              with open('bench-google.json', 'r') as f:
                  data = json.load(f)

              # Create directory
              os.makedirs('docs/documentation', exist_ok=True)
              
              # Start building content
              lines = []
              lines.append("# Continuouscl Benchmarking Results")
              lines.append("")
              lines.append(f"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}")
              lines.append("")
              lines.append("## System Information")
              lines.append(f"- Host: {data['context']['host_name']}")
              lines.append(f"- CPUs: {data['context']['num_cpus']}")
              lines.append(f"- MHz per CPU: {data['context']['mhz_per_cpu']}")
              lines.append("")
              lines.append("## Benchmark Results")
              lines.append("")
              lines.append("| Test Case | Metric | Time (seconds) | Time (nanoseconds) |")
              lines.append("|-----------|--------|----------------|-------------------|")
              
              # Add benchmark data
              for benchmark in data['benchmarks']:
                  name_parts = benchmark['name'].split('/')
                  case_name = name_parts[0]
                  metric_name = name_parts[1] if len(name_parts) > 1 else 'unknown'
                  time_seconds = benchmark['real_time'] / 1e9
                  time_ns = benchmark['real_time']
                  lines.append(f"| {case_name} | {metric_name} | {time_seconds:.6f} | {time_ns:.0f} |")
              
              lines.append("")
              lines.append("## Raw Data")
              lines.append("")
              lines.append("```json")
              lines.append(json.dumps(data, indent=2))
              lines.append("```")
              lines.append("")
              lines.append("---")
              lines.append(f"*Last updated: {datetime.now().isoformat()}*")
              
              # Write file
              with open('docs/documentation/cont-bench.md', 'w') as f:
                  f.write('\n'.join(lines))
              
              print("✓ Generated Markdown report at docs/documentation/cont-bench.md")

          if __name__ == "__main__":
              generate_markdown_report()
          SCRIPT

      - name: Generate Markdown Report
        run: python3 generate_report.py

      - name: Commit and Push Results
        run: |
          git config --local user.email "mohdsaid497566@gmail.com"
          git config --local user.name "Malmahrouqi3"
          
          # Fetch latest changes from the remote branch
          git fetch origin cont-bench || true
          
          # Stage the generated file
          git add docs/documentation/cont-bench.md
          
          # Commit changes to have a clean working tree
          git commit -m "Update continuous benchmarking results" || true
          
          # Check if we're behind the remote branch
          if git rev-list HEAD..origin/cont-bench --count | grep -q "^[1-9]"; then
            echo "Branch is behind remote, resolving..."
            
            # Save our changes to a temporary branch
            git branch temp-benchmark-updates
            
            # Get up to date with the remote branch
            git reset --hard origin/cont-bench
            
            # Regenerate the report with the latest code
            python3 generate_report.py
            
            # Add the newly generated report
            git add docs/documentation/cont-bench.md
            
            # Commit the changes
            git commit -m "Update continuous benchmarking results" || true
          fi
          
          # Push the changes, forcing if necessary
          git push origin HEAD:cont-bench || git push -f origin HEAD:cont-bench
                    
      - name: Store benchmark result
        uses: benchmark-action/github-action-benchmark@v1
        with:
          name: C++ Benchmark
          tool: 'googlecpp'
          output-file-path: bench-google.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: false
          alert-threshold: '200%'
          comment-on-alert: true
          fail-on-alert: true
          alert-comment-cc-users: '@Malmahrouqi'
